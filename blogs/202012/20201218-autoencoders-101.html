<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=content-widht, initial-scale=1.0">
  <meta name="description"
    content="Autoencoders are unsupervised learning technique in which we design neural networks for the task of representation learning">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <!-- Facebook Content -->
  <meta property="og:title" content="Introduction to Autoencoders | neuraltalks.io">
  <meta property="og:image" content="../../images/202012/20201218_Autoencoders_FB_OG.png">
  <title>Introduction to Autoencoders | neuraltalks.io</title>
  <link rel="icon" href="../../images/neuraltalks.ico">
  <!-- <link rel="stylesheet" href="/build/vendor.css"> -->
  <link href="https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css" rel="stylesheet">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <!-- <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
    integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
    integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
    crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
    integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EBPFJRPY24"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-EBPFJRPY24');
  </script>
  <style>
    .prose {
      color: #374151;
      max-width: 65ch;
      font-size: 1rem;
      line-height: 1.75;
      margin-top: 1.5em;
    }
  </style>
</head>

<body>
  <!-- HEADER -->
  <div class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0">
    <header class="flex justify-between items-center py-10">
      <div class="flex items-center">
        <a href="../../index.html" class="block">
          <object data="../../images/logo.svg" alt="neuraltalks.io" type="image/svg+xml"
            style="width:auto; height: 22px;"></object>
        </a>
      </div>
      <div class="text-base leading-5">
        <a href="../../index.html" class="font-medium text-gray-700 hover:text-gray-700">Blog</a>
        <a href="../../resume.html" class="font-medium text-gray-500 hover:text-gray-700 pl-4">About</a>
      </div>
    </header>
  </div>
  <!-- ./HEADER -->
  <!-- CONTENT -->
  <div class="max-w-3xl mx-auto px-4 sm:px-6 xl:max-w-5xl xl:px-0">
    <main>
      <article class="xl:divide-y xl:divide-gray-200">
        <header class="pt-6 xl:pb-10">
          <div class="space-y-1 text-center">
            <dl class="space-y-10">
              <div>
                <dt class="sr-only">Published on</dt>
                <dd class="text-base leading-6 font-medium text-gray-500">
                  <time>Friday, December 18, 2020</time>
                </dd>
              </div>
            </dl>
            <div
              class="text-3xl leading-9 font-extrabold text-gray-900 tracking-tight sm:text-4xl sm:leading-none md:text-5xl md:leading-none">
              Introduction to Autoencoders
            </div>
          </div>
        </header>

        <div class="divide-y xl:divide-y-0 divide-gray-200 xl:grid xl:grid-cols-4 xl:col-gap-6 pb-16 xl:pb-20"
          style="grid-template-rows: auto 1fr;">
          <dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200">
            <dt class="sr-only">Authors</dt>
            <dd>
              <ul class="flex justify-center xl:block space-x-8 sm:space-x-12 xl:space-x-0 xl:space-y-8">
                <li class="flex items-center space-x-2">
                  <img src="../../images/avatar.jpg" alt="Erik Dao" class="w-10 h-10 rounded-full">
                  <dl class="text-sm font-medium leading-5 whitespace-no-wrap">
                    <dt class="sr-only">Name</dt>
                    <dd class="text-gray-900">Erik Dao</dd>
                    <dt class="sr-only">Twitter</dt>
                    <dd>
                      <a href="http://twitter.com/@cuongddao" target="_blank"
                        class="text-red-500 hover:text-red-600">@cuongddao</a>
                    </dd>
                  </dl>
                </li>
              </ul>
            </dd>
          </dl>
          <div class="divide-y divide-gray-200 xl:pb-0 xl:col-span-3 xl:row-span-2">
            <div class="prose max-w-none pt-10 pb-8">
              <div class="rounded-xl mx-2 my-2 bg-red-100 text-red-800 p-4">
                Sample content. Texts are not my own.
              </div>
              <p>
                Autoencoders are an unsupervised learning technique in which we leverage neural networks for the task of
                representation learning. Specifically, we'll design a neural network architecture such that we impose a
                bottleneck in the network which forces a compressed knowledge representation of the original input. If
                the input features were each independent of one another, this compression and subsequent reconstruction
                would be a very difficult task. However, if some sort of structure exists in the data (i.e.,
                correlations between input features), this structure can be learned and consequently leveraged when
                forcing the input through the network's bottleneck.
              </p>

              <div class="prose">
                The ideal autoencoder balances the following:
                <ul class="list-disc">
                  <li class="ml-4">Sensitive to the input enough to accurately build a reconstruction</li>
                  <li class="ml-4">Insensitive enough to the input that the model doesn't simply mesmorize or overfit
                    training data
                  </li>
                </ul>
              </div>
              <div class="prose">
                To that end, the training objective function for an autoencoder is usually in the form
                \[
                \mathcal{L}\left( x, \hat{x} \right) + \beta * \text{regularizer}
                \]
              </div>

              <h2 class="text-2xl font-bold mt-6">Spare autoencoders</h2>
              <div class="prose">
                Spare autoencoders offer us an alternative method for introducing an information bottleneck without
                <span class="italic">requiring</span> a reduction in the number of nodes at our hidden layers. Rather,
                we'll construct our loss function such that we penalize <span class="italic">activations</span> within a
                layer. For any given observation, we'll encourage our network to learn an encoding and decoding which
                only relies on activating a small number of neurons. It's worth noting that this is a different approach
                towards regularization, as we normally regularize the <span class="italic">weights</span> of a network,
                not the activations.
              </div>
              <div class="prose">
                It's important to note that the individual nodes of a trained model which activate are <span
                  class="italtic">data-dependent</span>, different inputs will result in activations of different nodes
                through the network.
              </div>
              <div class="prose">
                One result of this fact is that <span class="font-bold">we allow our network to sensitize individual
                  hidden layer nodes toward specific attributes of the input data.</span> Whereas an undercomplete
                autoencoder will use the entire network for every observation, a sparse autoencoder will be forced to
                selectively activate regions of the network depending on the input data. As a result, we've limited the
                network's capability to extraect features from the data. This allows us to consider the latent state
                representation and regularization of the network <span class="italic">separately</span>, such that we
                can choose a latent state representation (i.e., encoding dimensionality) in accordance with what makes
                sense given the context of the data while imposing regularization by the sparsity constraint.
              </div>

              <div class="prose">
                There are two main ways by which we can impose this sparsity constraint; both involve measuring the
                hidden layer activations for each traeining batch and adding some term to the loss function in order to
                penalize excessive activations. These terms are:

                <ul class="list-disc mt-2">
                  <li class="ml-4 pl-3"><strong>L1 Regularization</strong>: We can add a term to our loss function that
                    penalizes the absolute value of the vector of activations \( a \) in layer \( h \) for observation
                    \( i \), scaled by a tuning parameters \( \lambda \).
                    \[
                    \mathcal{L}\left( x, \hat{x} \right) + \lambda \displaystyle \sum_{i=1} |a_i^{(h)}|
                    \]
                  </li>
                </ul>
              </div>

              <h2 class="text-2xl font-bold mt-4 mb-2">Denoising autoencoders</h2>
            </div>
          </div>
        </div>
      </article>
    </main>
  </div>
  <!-- ./CONTENT -->
</body>

</html>
